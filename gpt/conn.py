# Importing necessary libraries. 'os' is used for accessing environment variables and 'openai' for interacting with OpenAI's API.
import os
import openai

# Defining a function 'connect_to_openai' that takes two parameters: 'prompt_commands' and 'pdf_text'.
def connect_to_openai(prompt_commands, code_text):

    # Printing a message to indicate that a connection to OpenAI is being established.
    print("Connecting to OpenAI")

    # Setting the OpenAI API key. It retrieves the API key stored in the environment variable 'OPENAI_API_KEY'.
    openai.api_key = os.getenv('OPENAI_API_KEY')
    
    # Checking if the API key is present. If not, it raises a ValueError indicating the absence of the API key.
    if not openai.api_key:
        raise ValueError("No OpenAI API key found in environment variables")

    # Creating the prompt by concatenating the 'pdf_text' and 'prompt_commands'. The pdf_text is enclosed in quotes.
    prompt="\"" + code_text + "\"" + " " + prompt_commands
    
    message=[
        {"role": "system", "content": "You are a programming judge. You follow a specific methodology for deciding if the code passes or fails the test"},
        {"role": "user", "content": prompt},
    ]

    # Sending a request to the OpenAI API with specified parameters.
    response = openai.ChatCompletion.create(
        # Specifying the use of OpenAI's text-davinci-003 engine for generating responses.
        model="gpt-3.5-turbo",

        messages=message, 


        # Setting the temperature to 0, which means the model will provide deterministic, consistent responses.
        temperature=0.1,

        # Limiting the maximum number of tokens (words/pieces of words) in the response to 1500.
        max_tokens=1500,

        # Setting 'top_p' to 1, which means the model will consider the full range of possible next tokens at each step.
        top_p=1,

        # No frequency penalty is applied, allowing the model to repeat words or phrases.
        frequency_penalty=0,

        # No presence penalty is applied, not affecting the likelihood of new or existing content in the response.
        presence_penalty=0)

    # Returning the response generated by the OpenAI model.
    return response
